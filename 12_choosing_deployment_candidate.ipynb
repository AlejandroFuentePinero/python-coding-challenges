{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded0d722-ccd1-42a0-9213-73ffc4430114",
   "metadata": {},
   "source": [
    "# Choosing the Deployment Candidate\n",
    "\n",
    "### Background\n",
    "A small analytics startup has been benchmarking several prediction models.\n",
    "For each candidate, they recorded overall accuracy, average response time, a fairness penalty score (lower is better), and how many training samples were used.\n",
    "They now need to decide which models are actually acceptable for production, and among those, which ones are most attractive to deploy.\n",
    "The team has some “hard rules” and then a softer scoring idea, but nothing is coded yet.\n",
    "They’ve asked you to build a small decision helper on top of their benchmark summary.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. From the list of candidates, filter out any model that fails any of these hard requirements:\n",
    "\n",
    "    * accuracy must be at least 0.85\n",
    "\n",
    "    * latency must be at most 120 ms\n",
    "\n",
    "    * fairness penalty must be at most 0.10\n",
    "\n",
    "    * training samples must be at least 10,000\n",
    "\n",
    "2. For the remaining models, compute a simple “deployment score” that rewards accuracy, penalizes latency and fairness issues.\n",
    "You can design the exact formula yourself, but it should:\n",
    "\n",
    "    * increase when accuracy is higher\n",
    "\n",
    "    * decrease when latency or fairness penalty are higher\n",
    "\n",
    "3. Rank the acceptable models by this score, from best to worst, and print a small summary of the top three.\n",
    "\n",
    "4. Identify any dominated models among the acceptable ones: a model A is dominated by B if B is at least as good in all metrics (accuracy higher or equal, latency lower or equal, fairness penalty lower or equal, training samples higher or equal) and strictly better in at least one.\n",
    "Print any such dominated pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c72282e4-880f-4a8f-8d02-c27122bdabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATES = [\n",
    "    {\n",
    "        \"name\": \"M1\",\n",
    "        \"accuracy\": 0.86,\n",
    "        \"latency_ms\": 110,\n",
    "        \"fairness_penalty\": 0.05,\n",
    "        \"train_samples\": 12000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"M2\",\n",
    "        \"accuracy\": 0.83,\n",
    "        \"latency_ms\": 95,\n",
    "        \"fairness_penalty\": 0.03,\n",
    "        \"train_samples\": 18000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"M3\",\n",
    "        \"accuracy\": 0.89,\n",
    "        \"latency_ms\": 140,\n",
    "        \"fairness_penalty\": 0.02,\n",
    "        \"train_samples\": 25000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"M4\",\n",
    "        \"accuracy\": 0.91,\n",
    "        \"latency_ms\": 118,\n",
    "        \"fairness_penalty\": 0.09,\n",
    "        \"train_samples\": 9000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"M5\",\n",
    "        \"accuracy\": 0.88,\n",
    "        \"latency_ms\": 105,\n",
    "        \"fairness_penalty\": 0.11,\n",
    "        \"train_samples\": 16000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"M6\",\n",
    "        \"accuracy\": 0.92,\n",
    "        \"latency_ms\": 100,\n",
    "        \"fairness_penalty\": 0.04,\n",
    "        \"train_samples\": 20000,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"M7\",\n",
    "        \"accuracy\": 0.85,\n",
    "        \"latency_ms\": 115,\n",
    "        \"fairness_penalty\": 0.08,\n",
    "        \"train_samples\": 15000,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4217a77e-cdbe-4778-b16e-13927f067e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(CANDIDATES):\n",
    "    requirements_passed = []\n",
    "\n",
    "    print('Benchmark tests')\n",
    "    print('---------------\\n')\n",
    "\n",
    "    for i in CANDIDATES:\n",
    "        # Accuracy test\n",
    "        if i['accuracy'] < 0.85:\n",
    "            print(f'Model {i['name']} did not pass the accuracy test.\\nAccuracy = {i['accuracy']}\\n')\n",
    "        \n",
    "        # Latency test\n",
    "        elif i['latency_ms'] > 120:\n",
    "            print(f'Model {i['name']} did not pass the latency test.\\nLatency = {i['latency_ms']} ms\\n')\n",
    "\n",
    "        # Fairness test\n",
    "        elif i['fairness_penalty'] > 0.1:\n",
    "            print(f'Model {i['name']} did not pass the fairness test.\\nFairness penalty = {i['fairness_penalty']}\\n')\n",
    "\n",
    "        # Training test\n",
    "        elif i['train_samples'] < 10000:\n",
    "            print(f'Model {i['name']} did not pass the training sample test.\\nTraining samples = {i['train_samples']}\\n')\n",
    "        \n",
    "        else:\n",
    "            requirements_passed.append(i)\n",
    "\n",
    "    print('\\nComputing model scores')\n",
    "    print('----------------------\\n')\n",
    "    \n",
    "    # Deriving averages\n",
    "    accuracy_sum = 0\n",
    "    latency_sum = 0\n",
    "    fairness_sum = 0\n",
    "    sample_sum = 0\n",
    "    \n",
    "    for i in requirements_passed:\n",
    "        accuracy_sum += i['accuracy']\n",
    "        latency_sum += i['latency_ms']\n",
    "        fairness_sum += i['fairness_penalty']\n",
    "        sample_sum += i['train_samples']\n",
    "\n",
    "    \n",
    "    accuracy_avg = accuracy_sum / len(requirements_passed)\n",
    "    latency_avg = latency_sum / len(requirements_passed)\n",
    "    fairness_avg = fairness_sum / len(requirements_passed)\n",
    "    sample_avg = sample_sum / len(requirements_passed)\n",
    "\n",
    "    # Weights\n",
    "\n",
    "    w_acc = 0.8\n",
    "    w_samples = 0.2\n",
    "    w_lat = 0.2\n",
    "    w_fair = 0.8\n",
    "\n",
    "    model_scores = {}\n",
    "\n",
    "    for i in requirements_passed:\n",
    "\n",
    "        acc_ratio      = i['accuracy'] / accuracy_avg\n",
    "        lat_ratio      = i['latency_ms'] / latency_avg\n",
    "        fair_ratio     = i['fairness_penalty'] / fairness_avg\n",
    "        samples_ratio  = i['train_samples'] / sample_avg\n",
    "        \n",
    "        score = (\n",
    "            (acc_ratio ** w_acc)\n",
    "            * ((1 / lat_ratio) ** w_lat)\n",
    "            * ((1 / fair_ratio) ** w_fair)\n",
    "            * (samples_ratio ** w_samples)\n",
    "            )\n",
    "\n",
    "        model_scores[i['name']] = score\n",
    "\n",
    "        print(f\"Model: {i['name']} | Score: {score:.4f}\")\n",
    "\n",
    "    sorted_scores = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    req_dict = {m['name']: m for m in requirements_passed}\n",
    "\n",
    "    \n",
    "\n",
    "    print('\\nModel report')\n",
    "    print('------------\\n')\n",
    "\n",
    "    print('Top 3 models:')\n",
    "    print(f'\\t 1. {sorted_scores[0][0]}')\n",
    "    print(f'\\t\\t Score = {sorted_scores[0][1]}')\n",
    "    print(f'\\t\\t Accuracy = {req_dict[sorted_scores[0][0]]['accuracy']}')\n",
    "    print(f'\\t\\t Latency = {req_dict[sorted_scores[0][0]]['latency_ms']}')\n",
    "    print(f'\\t\\t Fairness penalty = {req_dict[sorted_scores[0][0]]['fairness_penalty']}')\n",
    "    print(f'\\t\\t Training samples = {req_dict[sorted_scores[0][0]]['train_samples']}')\n",
    "\n",
    "    print(f'\\t 2. {sorted_scores[1][0]}')\n",
    "    print(f'\\t\\t Score = {sorted_scores[1][1]}')\n",
    "    print(f'\\t\\t Accuracy = {req_dict[sorted_scores[1][0]]['accuracy']}')\n",
    "    print(f'\\t\\t Latency = {req_dict[sorted_scores[1][0]]['latency_ms']}')\n",
    "    print(f'\\t\\t Fairness penalty = {req_dict[sorted_scores[1][0]]['fairness_penalty']}')\n",
    "    print(f'\\t\\t Training samples = {req_dict[sorted_scores[1][0]]['train_samples']}')\n",
    "\n",
    "    print(f'\\t 3. {sorted_scores[2][0]}')\n",
    "    print(f'\\t\\t Score = {sorted_scores[2][1]}')\n",
    "    print(f'\\t\\t Accuracy = {req_dict[sorted_scores[2][0]]['accuracy']}')\n",
    "    print(f'\\t\\t Latency = {req_dict[sorted_scores[2][0]]['latency_ms']}')\n",
    "    print(f'\\t\\t Fairness penalty = {req_dict[sorted_scores[2][0]]['fairness_penalty']}')\n",
    "    print(f'\\t\\t Training samples = {req_dict[sorted_scores[2][0]]['train_samples']}')\n",
    "\n",
    "\n",
    "\n",
    "    def dominates(b, a):\n",
    "        \"\"\"\n",
    "        Return True if model b dominates model a.\n",
    "        b is better or equal in all metrics,\n",
    "        and strictly better in at least one.\n",
    "        \"\"\"\n",
    "        better_or_equal = (\n",
    "            b['accuracy']        >= a['accuracy'] and\n",
    "            b['train_samples']   >= a['train_samples'] and\n",
    "            b['latency_ms']      <= a['latency_ms'] and\n",
    "            b['fairness_penalty']<= a['fairness_penalty']\n",
    "        )\n",
    "\n",
    "        strictly_better = (\n",
    "            b['accuracy']        > a['accuracy'] or\n",
    "            b['train_samples']   > a['train_samples'] or\n",
    "            b['latency_ms']      < a['latency_ms'] or\n",
    "            b['fairness_penalty']< a['fairness_penalty']\n",
    "        )\n",
    "\n",
    "        return better_or_equal and strictly_better\n",
    "\n",
    "\n",
    "    dominated_pairs = []  # list of tuples (dominated_model, dominating_model)\n",
    "\n",
    "    for a in requirements_passed:\n",
    "        for b in requirements_passed:\n",
    "            if a['name'] == b['name']:\n",
    "                continue\n",
    "            if dominates(b, a):\n",
    "                dominated_pairs.append((a['name'], b['name']))\n",
    "\n",
    "    print(\"\\nDominance analysis\")\n",
    "    print(\"------------------\\n\")\n",
    "\n",
    "    if dominated_pairs:\n",
    "        for dom, dom_by in dominated_pairs:\n",
    "            print(f\"Model {dom} is dominated by model {dom_by}\")\n",
    "    else:\n",
    "        print(\"No dominated models found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a5346a3d-e3c1-46d1-b221-08180c282008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark tests\n",
      "---------------\n",
      "\n",
      "Model M2 did not pass the accuracy test.\n",
      "Accuracy = 0.83\n",
      "\n",
      "Model M3 did not pass the latency test.\n",
      "Latency = 140 ms\n",
      "\n",
      "Model M4 did not pass the training sample test.\n",
      "Training samples = 9000\n",
      "\n",
      "Model M5 did not pass the fairness test.\n",
      "Fairness penalty = 0.11\n",
      "\n",
      "\n",
      "Computing model scores\n",
      "----------------------\n",
      "\n",
      "Model: M1 | Score: 1.0288\n",
      "Model: M6 | Score: 1.4653\n",
      "Model: M7 | Score: 0.7253\n",
      "\n",
      "Model report\n",
      "------------\n",
      "\n",
      "Top 3 models:\n",
      "\t 1. M6\n",
      "\t\t Score = 1.4653450677842041\n",
      "\t\t Accuracy = 0.92\n",
      "\t\t Latency = 100\n",
      "\t\t Fairness penalty = 0.04\n",
      "\t\t Training samples = 20000\n",
      "\t 2. M1\n",
      "\t\t Score = 1.0288027314471755\n",
      "\t\t Accuracy = 0.86\n",
      "\t\t Latency = 110\n",
      "\t\t Fairness penalty = 0.05\n",
      "\t\t Training samples = 12000\n",
      "\t 3. M7\n",
      "\t\t Score = 0.7252595791677968\n",
      "\t\t Accuracy = 0.85\n",
      "\t\t Latency = 115\n",
      "\t\t Fairness penalty = 0.08\n",
      "\t\t Training samples = 15000\n",
      "\n",
      "Dominance analysis\n",
      "------------------\n",
      "\n",
      "Model M1 is dominated by model M6\n",
      "Model M7 is dominated by model M6\n"
     ]
    }
   ],
   "source": [
    "model_selection(CANDIDATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08575b66-a61d-4337-827b-a4036f29264f",
   "metadata": {},
   "source": [
    "This challenge brought together several core Python and data-science fundamentals in a practical, model-evaluation workflow. I implemented conditional filtering using loops and comparisons to determine which models passed the benchmark constraints, then used dictionary construction, ratio-based scoring, and weighted multiplicative formulas to rank the acceptable models. I also practiced computing dataset-level statistics (means), accessing nested dictionary fields, constructing sorted lists using sorted() with custom key functions, and formatting readable output reports. The final task introduced a classic multi-criteria comparison: dominance analysis, which required defining a helper function, iterating pairwise over models, and applying logical conditions to determine which models were strictly outperformed across all dimensions. Altogether, the challenge reinforced clean code structure, clarity of logic, multi-step reasoning, and practical evaluation patterns that mirror real-world model selection pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0b332-0eec-455c-a05e-8b13af0877f1",
   "metadata": {},
   "source": [
    "# === End of Challenge! ==="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
